# Nouvelle interface, nouvelles utilisations

### Faire une recherche, ou *de la* recherche 

Un des enjeux que nous souhaitons aborder avec la création d'une nouvelle interface de recherche pour CONBAVIL est le suivant: comment ne pas faire une recherche mais de la recherche avec une base de données? Nous commençons donc avec les enjeux épistémologiques sous-jacents à un projet de base de données en histoire de l'art. Ensuite, inversement, arrive la question de ce qu'on peut faire avec des données, et plus particulièrement avec les données de CONBAVIL? Finalement, le dernier élément nécessaire pour envisager une nouvelle interface est la préparation des données. Nous abordons donc la question des formats et entrons plus en détail dans la structuration des données dans la dernière sous-partie de ce chapitre. 

## Enjeux épistémologiques de la "remédiation" des archives ( )

Nous nous attaquons ici à la question des formes de production du savoir en histoire de l'art. S'il est commun d'utiliser les archives comme source, ont doit se demander quel est l'impact de la transformation de ces archives et les méthodes appropriées pour les "exploiter" lorsqu'elles prennent la forme d'une base de données. Nous utilisons le terme "remédiation" pour signaler le changement de médium et les enjeux de médiation sous-jacents. 

Comme nous l'avons vu dans les deux parties précédentes de ce chapitre, le processus employé dépasse largement une "simple" numérisation, ce qui équivaudrait à une reproduction numérique des documents d'archive (de leur image) [^1]. Le dépouillement analytique est un contenu interprété sous une forme nativement numérique. Si certaines données sont de réelles transcription du contenu des archives, d'autres sont le produit d'une interprétation des archives[^4]. 

<!-- faire lire à Christelle ? -->

Dans le but de définir le statut de cette source médiée, nous utilisons la théorie des "savoirs situés" de la chercheuse féministe Donna Haraway. Cette théorie déconstruit l'idée d'un "vrai" savoir. Dans les processus de production du savoir, une position qui n'est pas située se révèle être les perspectives normalisées de l'homme et du blanc - *the unmarked positions of Man and White* (Haraway, 1988: 581). Afin de déconstruire ces déclarations de vérités, il faut révéler les procédés scientifiques, les moyens techniques et les technologies sémiotiques à l'œuvre dans les productions du savoir (Haraway 1988: 582) <!-- à développer ou dire qu'on y reviendra dans chap 3? -->. "L'objectivité féministe signifie tout simplement des savoirs situés". De ce fait, "seules les perspectives partielles peuvent promettre une vision objective" (Haraway 1988: 583) 

Appliquer cette théorie à notre base de données nous permet de lui conférer plusieurs statuts. C'est l'utilisation faite de la base de données qui détermine son statut en tant que source. Il faut situer la production du savoir qui en émane pour identifier son rôle en tant que source primaire, secondaire ou médiée. Si l'objet d'étude est la base de données, alors son contenu est une source primaire. Dans le cas où l'objet d'étude est le contenu des archives du Conseil des bâtiments civils, deux positions, au moins, émergent. La première, lorsqu'on cite un rapport transcrit dans la base de données,se rapproche d une source primaire rééditée. La deuxième se base sur les interprétations des chercheur·se·s et serait, de ce fait, une source secondaire. 

Haraway rapporte également la présence des outils et instruments dont nous nous servons pour voiret savoir. "*Instruments of vision mediate standpoints*" (1988: 586). Il faut donc prendre conscience des nombreuses "épaisseurs" de médiation à l'œuvre. 

```
Histoire/passé/architecture publique en France au XIXe siècle | archives | base de données | interface de consultation


```

La situation pourrait encore devenir tout autre si l'on enrichit la base de données actuelle la reproduction numérique des archives. Ce travail supplémentaire [^6] changerait la donne car les utilisateur·rice·s de la base de données pourront citer leur source de façon beaucoup plus simple, tout en bénéficiant des avantages du dépouillement numérique. Il·elle·s pourront également poser un regard critique sur l'indexation et les champs qui sont le produit d'une interprétation. On atteindrait ainsi un certain idéal de transparence, même s'il ne faut pas se leurrer qu'il demeure de nombreuses couches de médiations. 

### L'utilisation de sources numériques pour la production de connaissance en histoire de l'art

Cela ne fait pas si longtemps que nous avons des sources numériques en histoire de l'art. Comme le fait remarquer Jacques Thuillier dans son article sur l'histoire de l'art et l'informatique en 1992, notre discipline est loin d'être un précurseur dans l'adaptation des innovations techniques à ses besoins et intérêts [^2]: "Un quart de siècle d'informatique n'a rien changé aux habitudes de l'histoire de l'art, tant internationale que française" (5). Il relève toutefois 

1) présence/existence de ces sources 

2) possibilité de les utiliser pour produire de nouveaux savoirs 



modification et évolutions des formes d'accès au savoir (que ce soit physiquement, à la salle des inventaire, avec les classements etc. ou autre, numérique etc)



"une seule réussite est évidente: la *B.H.A* (*Bibliographie d'histoire de l'art*) ... "

Passionnément souhaitée par André Chastel depuis 1969, aménagée par un mariage de raison entre le *R.A.A* (*Répertoire Art et Archéologie*, France, CNRS) et le *R.I.L.A* (*Répertoire International de Littérature*
*art*, U.S.A College Art Association), elle offre un instrument international qu'on peut certes critiquer sur certains détails secondaires, mais qui existe, fonctionne et rend service. Il s'agit là d'une réalisation dont les historiens d'art peuvent à juste titre se vanter. Mais il faut bien remarquer que la *B.H.A.* offre une bibliographie courante de l'histoire de l'art, où l'informatique a simplement ajouté ses facilités (et ses complications) à une bibliographie imprimée préexistante, selon des modèles communs à toutes les bibliographies scientifiques. 



CONTEXTE: 

- état de l'art: 1992 Thuillier
- Joyeux-Prunel 2008 

### Des humanités numériques et des données 

"La production de volumes considérables de données hétérogènes (big data) nécessit[e] le développement de nouvelles méthodes d'extraction et de traitement de l'information" (Gefen 2015: 61)

Données permettre une "approche empirique des faits où l'on peut extraire les phénomènes par des méthodes algorithmiques" (Gefen 2015: 61) <!-- algorithmiques ou computationnelles ? --> 



Humanitiés numériques: **Apparition d'un paradigme méthodologique et épistémologique qu'il importe de saisir dans toute sa puissante heuristique, sans se laisser entraîner par l'idée naïve d'une production transparente de savoirs par moissonnage du big data des corpus, masses de données qui restent des artefacts muets en l'absence d'une herméneutique spécifique** (Gefen 2015: 62) --> appliqué à l'histoire de l'art <!-- intro chap 2?-->



Interdisciplinarité --> cartographie  & visualisation de données (sémiologie, graphisme, développement d'interface)

Matteo Treleani, Qu'est-ce que la patrimoine numérique, chap 3: la circulation des archives

[finir écouter conférence Treleani]

nouvelle approche des archives: comme un tout <!-- à déplacer avec les datascapes dans le 2.3 ?? -->

complémente et enrichit l'approche classique en facilitant l'accès justement 





les humanités numériques (Gefen 2015: 61)

- tournant informationnel et computationnel du travail scientifique
- **production de volumes considérables de données hétérogènes (big data), nécessitant le développement de nouvelles méthodes d'extraction et de traitement de l'information** 
- la production massive de données [...] s'offre à une **approche empirique des faits où l'on peut extraire les phénomènes par des méthodes** algorithmiques à partir de masses de données en limitant l'interférence des présupposés 



Penser une nouvelle interface (v2) pour la recherche avec Conbavil, c'est, premièrement, la créer en pensant aux questions de recherche évoquées par les chercheur·se·s, impliqués dans la création de la base mais les autres aussi (**chap1**) <!-- faire référence et revenir au chap1-->





## Espaces et formes d'interprétation des données ( )

Que faire avec des donnnées, comment penser les données? 

- "Database as a symbolic form": constructing a narrive. How to create "interactive narratives? --> "How can our new abilities to store vast amounts of data, to automatically classify, index, link, search and instantly r**etrieve it lead to new kinds of narratives"** Manovich 1999





Caviglia - Design and the digital humanities @Séminaire "les fabriques cartographiques contemporaires" 2014

- l'invention ne se situe pas tant au niveau des innovation technologiques en imagerie informatique ou même des techniques de visualisations[...] que dans la manière dont ces techniques sont travaillées et mises en en sens par des praiques de design au contact des contextes effectifs, considérés dans toute leur épaisseur symbolique, psychologique et méthodologique
- statut des visualisation: ne sont jamais présentées comme des fins en soi ... mais plutôt comme une matière pour l'élaboration intellectuelle ̣- mouvante et incarnée dans divers modes d'inscrption, mobilisant en même temps les nouveaux outils et des séquences de pratiques et des protocloes de travails existants. Les images produites ne sont ainsi pas uniquement conçues comme des outils d'interprétation, mais aussi de tri, de paramétrage, voire même d'enrichissement et de transformation des données
- traduction des données "numérisées" en matières "digitales", desitnées - via le design d'interface et d'interaction, à utiliser le médium numérique comme un outil de manipulation et d'élaboration renouvelé dans le champs des Digital humanities

"The design of digital tools for scholarship is an intellectual responsibility, not a technical task" (Drucker 2009: B6) --> it is both intellectual and technical, bound by technology (Coleman, Mapping the republic of letters @Séminaire "les fabriques cartographiques contemporaires" 2014)

Donner matière à la construction d'une interface avec une approche bottom-up: partir des données pour construire une interface, même si tout au long, les questions de recherche et l'approche sont informées par le sujet (chap1). (Nos données sont aussi nos connaissances sur le passé)





### Analyse statistique ( )

fdfdfd





### Cartographie ( )

fddfdfs





## Travail préparatoire

Pour créer une nouvelle interface, il faut aussi penser aux formats et structures des données. Comme on l'a vu dans ces exemples, le format et la structure ont une forte influence sur ce qu'on peut faire par la suite. Il faut évidemment penser les deux ensemble, mais nous allons commencer par faire un survol des formats et de leur impact sur l'utilisation des données. 

<!-- ensuite, on va parler du type de recherche qu'on voudrait faire et comment elles fonctionnent: 2.3) -->

Nous avons déjà analysé un format de base de données dite à plat, TEXTO, et effectué quelques comparaisons avec la possibilité d'une base de données relationnelle. Dans l'ensemble, l'avantage des bases de données est qu'elles permettent d'enregistrer et de faire des requêtes sur un grand nombre de données. Cette efficacité est toutefois due à leur côté boîte noire, on ne voit jamais toutes les données. De plus, Filemaker étant un format propriétaire, il faut payer pour avoir le logiciel qui fonctionne sur un ordinateur. Le jour où la compagnie fait faillite et ne fait plus de mises à jour pour les nouveaux ordinateurs, ça devient très difficile à maintenir. Si l'utilisation à l'interne concerne le Centre André-Chastel, nous nous intéressons aux possibilités d'utilisations par les chercheur·se·s. Pour éviter les contraintes d'accès et d'interopérabilité, nous avons commencé par exporter les données.

>  Database format: *Being able to download this dataset influenced my perception of a digital collection. Even though I had access to exact copies of archival databases, they still made it difcult to consider a dataset as a single entity, as a whole. **In a database, information is scattered across a number of tables and in order to retrieve it, one has to formulate a precise query** [even when it isn't, it doesn't afford visualisation of the whole]. Many cultural institutions mirror this paradigm of interacting with a collection online by providing search forms and APIs that return only a limited number of records. On GitHub, it is possible to download an entire digital collection as a single CSV file. In principle, one could create a single file export from any database, but this is not something the database paradigm afords – in the Gibsonian (1977) sense. **Databases aford partial access, while downloading a file entails that all data is contained within that file**. A study conducted by Harper et al. (2013) highlights how users see files as something they can own and manipulate, giving them a sense of control and completeness – both qualities that are useful also for analysing data.* (Kraütli 2016: 145)



### Exportation des données

Pour des questions de conservation et de pérennité, nous avons tout d'abord choisi d'exporter les données dans le format XML (eXtensible Markup Language). Étant un simple fichier texte, ce format présente l'avantage d'être lisible sur tous les ordinateurs sans prérequis logiciel. Cela le rend plus résilient face aux évolutions des technologies. Il peut aussi être structuré lorsqu'on l'utilise avec un schéma, un ensemble de règles concernant le document. Pour "trouver" des données dans ce grand fichier linéaire, on utilise le langage XPATH qui trace un chemin vers les éléments qui correspondent à la requête. Il existe également le système de transformation de ces données, nommé XSLT (eXtensible Stylesheet Language Transformation), qui prend les informations et les retranscrit selon le format et la structure désirée. Finalement, c'est un langage qui est lisible par les machines comme par les humains. L'export des données Filemaker en un fichier XML avec un schéma correspondant a été effectué par Emmanuel Château-Dutier dans le cadre de ses recherches sur le conseil des bâtiments civils et les données CONBAVIL. Nous avons ainsi obtenu un premier accès aux données. 

Une second élément important l'accès à la nouvelle interface. Tout comme l'outil d'interrogation, nous pensons que pour rendre cette base de données accessible, elle doit être sur le web. En ce qui concerne le web, il est préférable d'utiliser le format JSON (JavaScript Object Notation). C'est un format de données qui est pris en charge nativement dans les standards du web: le trio HTML, CSS et JavaScript. Dans ce trio, HTML contient le contenu textuel, CSS sert à la mise en page et Javascript à la gestion des interactions telles que des mouvements de souris et des clics. 

S'il existe de très nombreuses options de format pour créer une interface de consultation des données CONBAVIL, nous avons choisi d'utiliser uniquement le trio standard du web pour plusieurs raisons. Tout d'abord, nous voulons limiter les dépendances à des logiciels ou à des outils de programmation afin d'assurer un maximum d'autonomie au projet [^12].  

- minimal (limiter les dépendances, augmenter l'autonomie)
- volonté de mettre le temps et l'effort dans l'exploration des visualisations avec d3.js
- simplicité : ceci est une expérimentation, il fallait essayer de limiter la complexité pour pouvoir se rendre à un résultat intéressant (dur à justifier, rien ne dit que j'ai pris les bonnes décisions de ne pas utiliser React par exemple)

<!-- aucune idée comment expliquer tout: ce que c'est PHP, les autres formats, CMS, react, .... il y a beaucoup trop à dire et rien qui semble si pertinent en soit -->

inspiration, dès le départ, de créer une interface (web) interactive et de visualiser les données (aller plus loin que le contenu texte). 

La chaîne de production est donc la suivante: travail à l'interne sur la base de données Filemaker, exportation vers XML pour la conservation des données, puis transformation en JSON pour leur utilisation web. Les scripts qui permettent de passer d'un format à l'autre sont faits pour être réutilisés et adaptés en cas de mise à jour dans les données. 

Nous avons donc écrit un script (avec l'aide de Stéfan Sinclair) pour transformer le fichier XML en JSON. Les propriétés sont structurées différemment et certains noms ont été changés car ce travail a été effectué avec l'appui de collaborateurs et à un stade nous n'étions pas très familiers avec les données. Le choix des nom de propriété est un peu aléatoire, mais ils peuvent facilement être changés dans le script si désiré. Ensuite, il a fallut régulariser plusieurs éléments de la base de données, c'est qu'on appelle le nettoyage des données. Il a notamment été nécessaire de séparer les marqueurs d'incertitude du contenu concerné. À l'écrit, on a tendance à mettre les choses entre parenthèses ou crochet, ou encore à ajouter un point d'interrogation. Cependant, pour les machines, il vaut mieux les indiquer de la même façon, et de préférence distinguer cette information du contenu textuel. Nous avons donc retiré tous ces marqueurs pour les remplacer par une propriété supplémentaire: "unsure-" dont la valeur est vraie quand le texte comportait une indication d'incertitude. Cela permet d'uniformiser les données sans pour autant perdre cette information importante. Nous avons aussi mis à jour les références vers le Thésaurus de la désignation des œuvres architecturales, car il été modifié et ne correspondait plus tout à fait aux entrées dans CONBAVIL. 

### Enrichir les données

L'aspect spatial important dans CONBAVIL nous a aussi tout de suite inspiré une approche cartographique. C'est pourquoi il fallait compléter les noms de communes, dûment ajustés aux divisions de la France actuelle par les checheur·se·s du CAC, par leur géolocalisation. Pour ce faire, nous avons tout d'abord fait une liste de toutes les communes mentionnées dans CONBAVIL. Nous avons ensuite cherché un fichier de données ouvertes qui contient la géolocalisation de toutes les communes françaises. Il a ainsi été possible de trouver de compléter la liste des communes de leur géolocalisation. Il reste certains problèmes d'encodage (la façon dont on écrit les accent) qui causent des erreurs dans l'alignement des données. Pour ce qui est des emplacements qui ne font pas partie de la France actuelle, les données comportaient le nom et le pays actuel. Nous avons utilisé un service de géolocalisation d'Open Street Map intitulé "Nominatim" pour obtenir les géolocalisations de ces lieux.

Une fois cette liste des communes enrichie des géolocalisations, nous avons ajouté les géolocalisations de ces communes pour l'ensemble des mentions de communes de CONBAVIL. Nous avons cependant fait face à des difficultés majeures, causées par une absence ou une perte de structuration des données au fil des changements. En effet, de nombreuses délibérations ne concernent pas un seul emplacement, mais plusieurs. Pour documenter cela, les chercheur·se·s ont entré les différentes communes, généralement séparées par des points virgules. Cependant, comme la propriété commune est un champ distinct de celle "département" et "numéro de département", il·elle·s ont également dû entrer ces informations dans leurs champs respectifs, à nouveau séparés par des points virgules lorsqu'il y en avait plusieurs. Le problème auquel nous avons fait face est la relative fréquence des homonymies entre les communes de départements distincts. Il s'est avéré à de multiples reprises qu'il n'était pas possible de déduire informatiquement quelle commune correspondait à quel département, n'ayant pas trouvé d'ordre entre les champs et leurs entrées respectives (quelle commune correspond à quel département et numéro de département). Malgré ces difficultés, nous avons réussi à compléter la géolocalisation de 22'000 délibérations <!-- reprendres le chiffres -->. Il nous semble possible de compléter celles manquantes, toutefois, dû aux contraintes temporelles, cela n'a pas été fait pour le moment.



### Vue distante du contenu (  )

Qu'est-ce que le contenu de la base de données CONBAVIL? Comment comprendre une base de données dans son ensemble? Son fonctionnement qui évoque une boîte noire fait son efficacité (on ne charge pas tout) mais mystifie le contenu car on ne peut pas "le voir".







En exportant les données, on peut s'outiller pour produire des vues[^15] sur l'ensemble de la base de données. Il existe par exemple le concept de *datascapes* ou "paysage de données", qui est "à la fois outil et méthode d'analyse, de visualisation et d'exploration d'archives" (Leclerc et Girard, 2017: 45). L'idée est de produire une sorte de topographie des données. Cependant, en lieu des latitudes, longitudes, altitudes et autres éléments figurés se trouvent les données et leur propriétés. 

Nous avons produit deux paysages de données pour expérimenter avec le concept ainsi que pour découvrir le contenu général de CONBAVIL. Le premier est un graphique très simple que l'on peut qualifier de fonctionnel [Annexe en ligne: datascape1]. C'est un histogramme, ou *bar chart*: chaque barre figure une propriété et sa hauteur est calculée par sa récurrence dans la base de données. Lorsque la propriété contient 1 élément pour chaque fiche, le résultat est de 26954. On peut prendre ce chiffre comme référence. 

<!-- trouver la formulation pour dire: Cependant, c'est une somme simple, ce qui signifie que  ce n'est pas "distribué" entre celles qui sont vides et celles qui contiennent plusieurs entrées [ask mom how it's called]:  techniquement, il pourrait y avoir 27'000 de quelque chose qui concerne 9000 délibérations avec en moyenne 3 entrées par exemple.  -->s

Le système bicolore permet de distinguer les occurrences uniques (en rose) de celles multiples. Logiquement, les champs normalisés sont majoritairement en bleu tandis que les champs plein texte sont en rose. Il faut noter que le champ "id" est nécessairement rose puisque ce sont les identifiants uniques. En bleu, le champ "file" concerne les 66 registres et le champs meeting sont les 4299 séances dont les procès-verbaux ont étés conservés et dépouillés. Puisqu'il s'agit d'un moyen figurer la base de données, nous n'avons pas retiré les champs moins pertinents, comme "id" par exemple, car c'est plus un exercice de figuration (brute ou directe) que d'expression analytique.

L'interactivité du graphique permet d'explorer le contenu de chaque barre en cliquant dessus. Cela affiche en dessous le nombre d'occurrences total, ainsi que les valeurs distinctes, puis une liste de ces dernières. 

![](/home/lenamk/Documents/atlasNumerique/Redaction/img/datascape_propriétés.png)



Le second datascape est un graphique complexe et long, plutôt conceptuel ou poétique[Annexe en ligne: datascape 2]. Il s'agit d'une trame composée des propriétés dont chaque ligne est une fiche. Ce système binaire, qui signale si la propriété est renseignée (sombre) ou non (clair), est inspiré par les matrices de l'éminent cartographe et sémiologue français, Jaques Bertin[^14]. Cela crée un motif qui évoque la texture de la dentelle ou les partitions à trous d'une boîte à musique.

Nous avons augmenté la transparence des champs que nous avons créés lors du nettoyage des données car ces champs n'existent pas en tant que tel dans la base (voir chp 2.2.2). Les propriétés sont regroupées thématiquement (toponymie, rapports et avis, détails)

<!-- Il faut l'imprimer pour voir l'ensemble et pouvoir parler de dimension + faire des extraits de différentes parties, pour voir si du début à la fin ça a beaucoup changé -->

![](/home/lenamk/Documents/atlasNumerique/Redaction/img/datascape.png)

<!-- à rédiger -->

Le principe des matrices de Bertin: système visuel de classification, la diagonalisation, qui permet d'identifier des similarités entre des entités 

https://aviz.fr/bertifier --> print + read + comment

notion de récurrence (ce qui est souvent rempli, ce qui ne l'est pas)





## Notes

[^1]: Nous avons mis  "simple" entre guillemets car si le fond reste quasi-identique via reproduction photographie haute résolution, la forme matérielle change. Cela implique la question de la classification, comment stocker et référencer ces archives numérisées.
[^2]: À contrario, parmi les institutions culturelle, les bibliothèques ont été très rapides pour envisager l'apport conséquent que l'informatique pouvait apporter à leur domaine
[^3]: 
[^4]: C.f. la création de la grille et l'entrée des données, la partie 2.1.1 de ce mémoire
[^6]: prévu par Emmnauel et les AN? 