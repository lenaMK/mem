# Nouvelle interface, nouvelles approches

## Enjeux épistémologiques 

Maintenant que nous avons des données, un des enjeux que nous souhaitons aborder avec la création d'une nouvelle interface de recherche pour CONBAVIL est le suivant: comment ne pas faire une recherche mais de la recherche avec une base de données? Nous commençons donc avec les enjeux épistémologiques sous-jacents à un projet de base de données en histoire de l'art. Ensuite, inversement, arrive la question de ce qu'on peut faire avec des données, et plus particulièrement avec nos données de CONBAVIL? Finalement, le dernier élément nécessaire pour envisager une nouvelle interface est la préparation des données. Nous abordons donc la question des formats et entrons plus en détail dans la structuration des données dans la dernière sous-partie de ce chapitre. 

### Des données aux humanités numériques

Il ne suffit pas d'avoir affaire à des données en sciences humaines pour se situer dans le champs des humanités numériques. Alexandre Gefen, <!-- chercheur en littérature et en humanités numériques? trop de répétition-->, suggère plutôt de concevoir les humanités numériques comme 

> "l'apparition d'un paradigme méthodologique et épistémologique qu'il importe de saisir dans toute sa puissante heuristique, sans se laisser entraîner par l'idée naïve d'une production transparente de savoirs par moissonnage du big data des corpus, masses de données qui restent des artefacts muets en l'absence d'une herméneutique spécifique" (Gefen 2015: 62). 

Dans le cas de grands volumes de données, il devient nécessaire de créer de nouveaux moyens pour les étudier et les traiter l'information (Gefen 2015: 61). En fait, même dans le cas de "petites" quantités de données, on peut distinguer les approches "numérisées" de celles "numériques" (Drucker ...) . Dans le premier cas, la numérisation change le médium sans pour autant affecter la méthode. Les chercheur·se·s effectuent sur l'ordinateur un travail qui serait équivalent à celui analogue. On pourrait parler d'une approche numérisée lorsque l'on "feuillette" des archives numérisées, sans autres données ou informations que l'ordre des pages. Pour explorer des méthodes numériques, le changement de médium des archives (qui dans ce cas ne sont pas nativement numériques...) est accompagné d'outils comme l'OCR ou d'une autre forme d'extraction des données qui métamorphose le contenu et les moyens disponibles pour la production de connaissance.

CONBAVIL n'est pas ce qu'on considère du *big data*, c'est-à-dire une "masse de données hétérogènes". C'est une forme d'archives numérique où l'extraction de données manuelle par des chercheur·se·s détient un riche potentiel épistémologique. De part la forme choisie pour ces données et leur quantité, il a, dès le départ, été nécessaire d'avoir un outil qui permette leur consultation et leur manipulation. Cet outil a pour but de répondre aux questions de recherche évoquées par les chercheur·se·s (2.2) mais aussi à celles actuelles dans la discipline (chapitre 1). Dans le contexte de création d'une seconde interface CONBAVIL, nous pensons toutefois qu'il est possible d'explorer et de prendre avantage d'un plus grand nombre d'opportunités offertes par le champs des humanités numériques. 

### La remédiation des archives

Nous reprenons ici plus en détail à la question des formes de production du savoir en histoire de l'art. S'il est commun d'utiliser les archives comme source, ont doit se demander quel est l'impact de la transformation de ces archives et les méthodes appropriées pour les "exploiter" lorsqu'elles prennent la forme d'une base de données. Nous utilisons le terme "remédiation" pour signaler le changement de médium et les enjeux de médiation sous-jacents. 

Comme nous l'avons vu dans les deux parties précédentes de ce chapitre, le processus employé dépasse largement une "simple" numérisation, ce qui équivaudrait à une reproduction numérique des documents d'archive (de leur image) [^1]. Le dépouillement analytique est un contenu interprété sous une forme nativement numérique. Si certaines données sont de réelles transcription du contenu des archives, d'autres sont le produit d'une interprétation des archives[^4]. 

<!-- faire lire à Christelle ? -->

Dans le but de définir le statut de cette source médiée, nous utilisons la théorie des "savoirs situés" de la chercheuse féministe Donna Haraway. Cette théorie déconstruit l'idée d'un "vrai" savoir. Dans les processus de production du savoir, une position qui n'est pas située se révèle être les perspectives normalisées de l'homme et du blanc - *the unmarked positions of Man and White* (Haraway, 1988: 581). Afin de déconstruire ces déclarations de vérités, il faut révéler les procédés scientifiques, les moyens techniques et les technologies sémiotiques à l'œuvre dans les productions du savoir (Haraway 1988: 582) <!-- à développer ou dire qu'on y reviendra dans chap 3? -->. "L'objectivité féministe signifie tout simplement des savoirs situés". De ce fait, "seules les perspectives partielles peuvent promettre une vision objective" (Haraway 1988: 583) 

Appliquer cette théorie à notre base de données nous permet de lui conférer plusieurs statuts. C'est l'utilisation faite de la base de données qui détermine son statut en tant que source. Il faut situer la production du savoir qui en émane pour identifier son rôle en tant que source primaire, secondaire ou médiée. Si l'objet d'étude est la base de données, alors son contenu est une source primaire. Dans le cas où l'objet d'étude est le contenu des archives du Conseil des bâtiments civils, deux positions, au moins, émergent. La première, lorsqu'on cite un rapport transcrit dans la base de données,se rapproche d une source primaire rééditée. La deuxième se base sur les interprétations des chercheur·se·s et serait, de ce fait, une source secondaire. 

Haraway rapporte également la présence des outils et instruments dont nous nous servons pour voiret savoir. "*Instruments of vision mediate standpoints*" (1988: 586). Il faut donc prendre conscience des nombreuses "épaisseurs" de médiation à l'œuvre. 

```
Histoire/passé/architecture publique en France au XIXe siècle | archives | base de données | interface de consultation
```

La situation pourrait encore devenir tout autre si l'on enrichit la base de données actuelle la reproduction numérique des archives. Ce travail supplémentaire [^6] changerait la donne car les utilisateur·rice·s de la base de données pourront citer leur source de façon beaucoup plus simple, tout en bénéficiant des avantages du dépouillement numérique. Il·elle·s pourront également poser un regard critique sur l'indexation et les champs qui sont le produit d'une interprétation. On atteindrait ainsi un certain idéal de transparence, même s'il ne faut pas se leurrer qu'il demeure de nombreuses couches de médiations. 

### L'utilisation de sources numériques pour la production de connaissance en histoire de l'art


Jacques Thuillier, dans son article sur l'histoire de l'art et l'informatique en 1992, sermonne notre discipline de n'avoir su être un précurseur dans l'adaptation des innovations techniques à ses besoins et intérêts [^2]: "Un quart de siècle d'informatique n'a rien changé aux habitudes de l'histoire de l'art, tant internationale que française" (5). De la *Bibliographie d'histoire de l'art*, "seule réussite [...] évidente" relevée par Thuillier en 1992 à aujourd'hui, la création et l'utilisation de sources numériques ne cesse d'augmenter, de se diversifier et d'être encouragée (Joyeux-Prunel: ?). Malgré un côté encore marginal à ces approches, on peut constater des modifications et des évolutions importantes dans les formes d'accès aux archives et dans la production du savoir. 

Faire une historiographie complète des projets de recherche innovants dans la production d'interfaces de recherche dépasse les limites de ce mémoire. L'affluence de projets et leur diversité pose la question s'ils peuvent être recensés de manière systématique à l'internationale. Notons qu'il existe déjà certains recensements très riches dont le travail de Johanna Drucker dans *Graphesis* (2013?). Afin de citer ceux particulièrement pertinents dans le cas de ce mémoire, nous les citons à titre d'exemples au long de ce chapitre. Ces cas situent notre travail tout comme ils se trouvent parfois à la source de notre inspiration pour la création de notre interface. 

Un cas actuel et exemplaire d'utilisation d'archives numériques concerne les archives d'une organisation, *Experiments in Arts and Technology* (E.A.T), créée en 1966 par les artistes Robert Rauschenberg et Robert Whitman et les ingénieurs Billy Klüver et Fred Waldhauer (Leclercq 2016: 46). Une équipe interdisciplinaire, au croisement de l'histoire de l'art sociale, du design et des humanités numériques, s'est penchée sur les archives E.A.T.. Les chercheurs du médialab ont commencé par identifier et interpréter des informations contenues dans les archives pour former un ensemble de données structurées. Après ce processus d'extraction [manuelle et humaine] des données, l'objectif a été de d'équiper les historien·ne·s d'un instrument de recherche pour les explorer (Leclerq et Girard, 2013: 6). 


Le résultat obtenu prend la forme d'un *datascape*, "à la fois outil et méthode d'analyse, de visualisation et d'exploration d'archives" (Leclercq, 2016: 45). L'interface interactive, disponible [en ligne](http://eat_datascape.medialab.sciences-po.fr/), offre la possibilité d'explorer les données sous plusieurs perspectives. Elle permet de "visualiser et [de] penser ensemble, et non comme séparés, processus et résultat, pour rendre compte de la complexité d'une réalisation à la vie particulièrement longue" (Leclercq, 2016: 50). Le *datascape* du projet E.A.T., tout comme celui produit pour étudier *Les transformations de l’économie française par le prisme du commerce international, 1716-1821* (TOFLIT18) (Loïc Charles, Guillaume Daudin, Guillaume Plique et Paul Girard, site web TOFLIT18. Consulté le 14 janvier 2021 http://toflit18.medialab.sciences-po.fr), démontrent avec aisance l'apport d'un tel instrument à la production de connaissance.



## Travail des données

> "How can our new abilities to store vast amounts of data, to automatically classify, index, link, search and instantly retrieve it lead to new kinds of narratives" (Manovich 1999: ?)

Quels moyens existe-t-il employer pour bâtir un *narrative* ou écrire l'histoire avec les données? Face à ce nouvel enjeu, il est nécessaire d'innover dans nos façon de penser et de produire des connaissances. Un défi particulier se situe dans l'interdisciplinarité de cette question. Si l'épistémologie appartient aux chercheur·se·s, les outils informatiques reviennent souvent à des "ingénieurs". Cette division des rôles et des tâches freine voire empêche l'innovation. Johanna Drucker argumente que "le design d'outil numériques pour la recherche est une responsabilité intellectuelle et non une tâche technique" (Drucker 2009: B6). Cela requiert soit une proche collaboration, soit une double formation, afin de saisir les enjeux de recherche tout en choisissant et en adaptant les technologies appropriées pour y répondre. En les considérant "dans toute leur épaisseur symbolique, psychologique et méthodologique" (Caviglia 2014 *Design and the digital humanities @Séminaire "les fabriques cartographiques contemporaires"*), on peut alors parvenir à produire de nouveaux outils et de nouvelles connaissances. 


La création d'interface et l'interactivité transforme des données numériques en une "matière digitale" que les chercheur·se·s peuvent manipuler (Caviglia 2014). En fait, lors du travail avec une base de données, une première étape importante est la familiarisation avec le contenu. 

>  "Being able to download this dataset influenced my perception of a digital collection. Even though I had access to exact copies of archival databases, they still made it difcult to consider a dataset as a single entity, as a whole. **In a database, information is scattered across a number of tables and in order to retrieve it, one has to formulate a precise query** [even when it isn't, it doesn't afford visualisation of the whole]. [...] 
>
>  In principle, one could create a single file export from any database, but this is not something the database paradigm afords – in the Gibsonian (1977) sense. **Databases aford partial access, while downloading a file entails that all data is contained within that file**. A study conducted by Harper et al. (2013) highlights how users see files as something they can own and manipulate, giving them a sense of control and completeness – both qualities that are useful also for analysing data." (Kraütli 2016: 145)

Il faut aussi effectuer un survol des formats et de leur impact sur l'utilisation des données. Nous avons déjà analysé un format de base de données dite à plat, TEXTO, et effectué quelques comparaisons avec la possibilité d'une base de données relationnelle. Dans l'ensemble, l'avantage des bases de données est qu'elles permettent d'enregistrer et de faire des requêtes sur un grand nombre de données. Cette efficacité est toutefois due à leur côté boîte noire, on ne voit jamais toutes les données. De plus, Filemaker étant un format propriétaire, il faut payer pour avoir le logiciel qui fonctionne sur un ordinateur. Le jour où la compagnie fait faillite et ne fait plus de mises à jour pour les nouveaux ordinateurs, ça devient très difficile à maintenir. Si l'utilisation à l'interne concerne le Centre André-Chastel, nous nous intéressons aux possibilités d'utilisations par les chercheur·se·s. Pour éviter les contraintes d'accès et d'interopérabilité, ainsi que pour manipuler l'ensemble du contenu CONBAVIL, nous avons commencé par exporter les données.

### Exportation et transformation

Pour des questions de conservation et de pérennité, nous avons tout d'abord choisi d'exporter les données dans le format XML (eXtensible Markup Language). Étant un simple fichier texte, ce format présente l'avantage d'être lisible sur tous les ordinateurs sans prérequis logiciel. Cela le rend plus résilient face aux évolutions des technologies. Il peut aussi être structuré lorsqu'on l'utilise avec un schéma, un ensemble de règles concernant le document. Pour "trouver" des données dans ce grand fichier linéaire, on utilise le langage XPATH qui trace un chemin vers les éléments qui correspondent à la requête. Il existe également le système de transformation de ces données, nommé XSLT (eXtensible Stylesheet Language Transformation), qui prend les informations et les retranscrit selon le format et la structure désirée. Finalement, c'est un langage qui est lisible par les machines comme par les humains. L'export des données Filemaker en un fichier XML avec un schéma correspondant a été effectué par Emmanuel Château-Dutier dans le cadre de ses recherches sur le conseil des bâtiments civils et les données CONBAVIL. Nous avons ainsi obtenu un premier accès aux données. 

Une second élément important l'accès à la nouvelle interface. Tout comme l'outil d'interrogation, nous pensons que pour rendre cette base de données accessible, elle doit être sur le web. En ce qui concerne le web, il est préférable d'utiliser le format JSON (JavaScript Object Notation). C'est un format de données qui est pris en charge nativement dans les standards du web: le trio HTML, CSS et JavaScript. Dans ce trio, HTML contient le contenu textuel, CSS sert à la mise en page et Javascript à la gestion des interactions telles que des mouvements de souris et des clics. 

S'il existe de très nombreuses options de format pour créer une interface de consultation des données CONBAVIL, nous avons choisi d'utiliser uniquement le trio standard du web pour plusieurs raisons. Tout d'abord, nous voulons limiter les dépendances à des logiciels ou à des outils de programmation afin d'assurer un maximum d'autonomie au projet [^12]. Cela permet aussi de maximiser le temps disponible pour travailler sur les visualisations de données. Comme il s'agit d'une expérimentation dans le cadre d'un mémoire de maîtrise et que l'apprentissage de nouvelles méthodes en programmation est chronophage, nous avons choisi de limiter la complexité des outils employés. Malgré beaucoup d'hésitation et de conseils divers, React.js, une librairie de code qui assiste la création d'interfaces utilisateur·rice·s, n'a donc pas été utilisé. Le point central du travail consiste de la librairie de code d3.js permet de créer des résultats visuels et interactifs, car l'inspiration, dès, dès le départ, était de créer une interface web visuelle et interactive.

Au vu de ces décision, la chaîne de production se présente comme suit: travail à l'interne sur la base de données Filemaker, exportation vers XML pour la conservation des données, puis transformation en JSON pour leur utilisation web. Les scripts qui permettent de passer d'un format à l'autre sont faits pour être réutilisés et adaptés en cas de mise à jour dans les données. 

Nous avons donc écrit un script (avec l'aide précieuse de Stéfan Sinclair) pour transformer le fichier XML en JSON. Les propriétés sont structurées différemment et certains noms ont été changés car ce travail a été effectué avec l'appui de collaborateurs et à un stade nous n'étions pas très familiers avec les données. Le choix des nom de propriété est un peu aléatoire, mais ils peuvent facilement être changés dans le script si désiré. Ensuite, il a fallut régulariser plusieurs éléments de la base de données, c'est qu'on appelle le nettoyage des données. Il a notamment été nécessaire de séparer les marqueurs d'incertitude du contenu concerné. À l'écrit, on a tendance à mettre les choses entre parenthèses ou crochet, ou encore à ajouter un point d'interrogation. Cependant, pour les machines, il vaut mieux les indiquer de la même façon, et de préférence distinguer cette information du contenu textuel. Nous avons donc retiré tous ces marqueurs pour les remplacer par une propriété supplémentaire: "unsure-" dont la valeur est vraie quand le texte comportait une indication d'incertitude. Cela permet d'uniformiser les données sans pour autant perdre cette information importante. Nous avons aussi mis à jour les références vers le Thésaurus de la désignation des œuvres architecturales, car il été modifié et ne correspondait plus tout à fait aux entrées dans CONBAVIL. 

### Enrichir les données

L'aspect spatial important dans CONBAVIL nous a aussi tout de suite inspiré une approche cartographique. C'est pourquoi il fallait compléter les noms de communes, dûment ajustés aux divisions de la France actuelle par les checheur·se·s du CAC, par leur géolocalisation. Pour ce faire, nous avons tout d'abord fait une liste de toutes les communes mentionnées dans CONBAVIL. Nous avons ensuite cherché un fichier de données ouvertes qui contient la géolocalisation de toutes les communes françaises. Il a ainsi été possible de trouver de compléter la liste des communes de leur géolocalisation. Il reste certains problèmes d'encodage (la façon dont on écrit les accent) qui causent des erreurs dans l'alignement des données. Pour ce qui est des emplacements qui ne font pas partie de la France actuelle, les données comportaient le nom et le pays actuel. Nous avons utilisé un service de géolocalisation d'Open Street Map intitulé "Nominatim" pour obtenir les géolocalisations de ces lieux.

Une fois cette liste des communes enrichie des géolocalisations, nous avons ajouté les géolocalisations de ces communes pour l'ensemble des mentions de communes de CONBAVIL. Nous avons cependant fait face à des difficultés majeures, causées par une absence ou une perte de structuration des données au fil des changements. En effet, de nombreuses délibérations ne concernent pas un seul emplacement, mais plusieurs. Pour documenter cela, les chercheur·se·s ont entré les différentes communes, généralement séparées par des points virgules. Cependant, comme la propriété commune est un champ distinct de celle "département" et "numéro de département", il·elle·s ont également dû entrer ces informations dans leurs champs respectifs, à nouveau séparés par des points virgules lorsqu'il y en avait plusieurs. Le problème auquel nous avons fait face est la relative fréquence des homonymies entre les communes de départements distincts. Il s'est avéré à de multiples reprises qu'il n'était pas possible de déduire informatiquement quelle commune correspondait à quel département, n'ayant pas trouvé d'ordre entre les champs et leurs entrées respectives (quelle commune correspond à quel département et numéro de département). Malgré ces difficultés, nous avons réussi à compléter la géolocalisation de 22'000 délibérations <!-- reprendres le chiffre -->. Il nous semble possible de compléter celles manquantes, toutefois, dû aux contraintes temporelles, cela n'a pas été fait pour le moment.



## Vue distante du contenu

Qu'est-ce que le contenu de la base de données CONBAVIL? Comment comprendre une base de données dans son ensemble? Son fonctionnement qui évoque une boîte noire fait son efficacité (on ne charge pas tout) mais mystifie le contenu car on ne peut pas "le voir". Avec l'exportation des données à laquelle nous avons procédé, on peut ensuite s'outiller pour produire des vues[^15] sur l'ensemble de la base de données. Nous reprenons le concept de *datascape* ou "paysage de données", qui est "à la fois outil et méthode d'analyse, de visualisation et d'exploration d'archives" (Leclerc et Girard, 2017: 45). L'idée est de produire une sorte de topographie des données de CONBAVIL. Cependant, en lieu des latitudes, longitudes, altitudes et autres éléments figurés se trouvent les données et leur propriétés. Nous avons produit deux paysages de données pour expérimenter avec le concept ainsi que pour découvrir le contenu général de CONBAVIL. 

### Datascape 1: occurences et valeurs distinctes de chaque propriété 

Le premier est un simple histogramme, ou *bar chart*. [Annexe en ligne: datascape1]. Chaque barre figure une propriété et sa hauteur est calculée selon sa récurrence dans la base de données. Le système bicolore permet de distinguer les occurrences uniques (en rose) de celles multiples. L'interactivité du graphique permet d'explorer le contenu de chaque barre en cliquant dessus. Cela affiche en dessous le nombre d'occurrences total, ainsi que les valeurs distinctes, puis une liste de ces dernières. 

![](/home/lenamk/Documents/atlasNumerique/Redaction/img/datascape_propriétés.png)

Logiquement, les champs normalisés sont majoritairement en bleu tandis que les champs plein texte sont en rose. Dû aux grandes disparités entre les valeurs <!-- on aurait pu mettre une échelle log--->, on ne peut pas voir la partie rose, c'est-à-dire les occurrences distinctes, de certaines propriétés comme les registres (valeur: 66) ou les catégories architecturales (16). Il faut noter que le champ "id" est nécessairement rose puisque ce sont les identifiants uniques. Puisqu'il s'agit d'un moyen figurer la base de données, nous n'avons pas retiré ce champs même s'il est moins pertinent, car c'est plus un exercice de figuration (brute ou directe) que d'expression analytique.

Lorsque la propriété contient 1 élément pour chaque fiche, le résultat est de 26954. C'est le cas pour la date de réunion. Il y a exactement une date par fiche, donc 26954 valeurs pour le champs date. Les valeurs distinctes nous informent que la base de données contient 4199 dates différentes. C'est une façon d'inférer que, sauf erreur [^ 7], c'est un poil moins de 4200 séances du conseil dont les procès-verbaux ont été numérisés.

Le type de bâtiment est un exemple d'une propriété est renseignée plusieurs fois par fiche. Le nombre d'occurrence monte à 43555, ce qui donne une moyenne de 1.5. Statistiquement parlant, on pourrait penser que la moitié des fiches comportent 1 type d'édifice, et l'autre moitié 2. Cependant, nous savons que plusieurs fiches ne comportent pas de type de bâtiment car il s'agit d'affaires administratives. Nous relevons donc ici l'utilité relative de ces chiffres. L'objectif est de donner une idée, mais à eux seuls, ces chiffres ne permettent pas de tirer des conclusions. Pour arriver à une évaluation statistique plus probante, il faudrait par exemple calculer la moyenne et les écarts-types. 

 <!-- ajouter exemple des dessins? 3665, Valeurs distinctes de la propriété: 3607. Si on regroupe par affaires, encore moins.. est-ce que les côtes des dessins permettraient de regrouper les affaires??? -->

S'il était évident avant de produire ce graphique que la base de données contenait les 66 premiers registres des procès-verbaux, il n'y a pas eu, à notre connaissance, d'étude "distante" du contenu de CONBAVIL. Ce type de graphique donne un contexte aux informations contenues dans la base de données. Il s'agit d'un outil utile pour situer des résultats (fiche classique, fiche hors normes etc.) dans l'ensemble des données.

### Datascape 2: approche sensible d'un million d'affirmations

27000 x 33 = 891'000

Le second datascape est un graphique complexe et long, plutôt conceptuel ou poétique [Annexe en ligne: datascape 2]. Il s'agit d'une trame composée des propriétés dont chaque ligne est une fiche. Ce système binaire, qui signale si la propriété est renseignée (sombre) ou non (clair), est inspiré par les matrices de l'éminent cartographe et sémiologue français, Jaques Bertin[^14]. Cela crée un motif qui évoque la texture de la dentelle ou les partitions à trous d'une boîte à musique.

<!-- Il faut l'imprimer pour voir l'ensemble et pouvoir parler de dimension + faire des extraits de différentes parties, pour voir si du début à la fin ça a beaucoup changé -->

![](/home/lenamk/Documents/atlasNumerique/Redaction/img/datascape.png)

<!-- à terminer: volonté de l'imprimer pour de vrai, voir ce qui est possible -->

Nous avons augmenté la transparence des champs que nous avons créés lors du nettoyage des données car ces champs n'existent pas en tant que tel dans la base (voir chp 2.2.2). Les propriétés sont regroupées thématiquement (toponymie, rapports et avis, détails)

Le principe des matrices de Bertin: système visuel de classification, la diagonalisation, qui permet d'identifier des similarités entre des entités --> travail / interpréation visuelle, capacité "instinctive" à ranger, classifier

- https://aviz.fr/bertifier --> print + read + comment
- notion de rythme, récurrence (ce qui est souvent rempli, ce qui ne l'est pas)
- exploration visuelle, sensorielle, des données --> matière à manipuler



Les paysages de données que nous avons créés sont pensés comme une étape de travail "préliminaire". Ils ne figurent pas le contenu en soit de CONBAVIL, mais plutôt des métadonnées. La différence entre données et métadonnées est une question de contexte. Dans ce cas, les données sont le contenu de chaque fiche, comme la date, l'avis, les topographie et typologie renseignées. Les métadonnées décrivent le contenu de la base de données, mais ne le contiennent pas. Par exemple, dans notre réinterprétation des matrices de Bertin, nous avons créé une métadonnées qui indiquent si "oui" ou "non" les propriétés, telle que la date, l'avis etc., sont renseignées dans la base de données. 



[à terminer + conclusion]



## Notes

[^1]: Nous avons mis "simple" entre guillemets car si le fond reste quasi-identique via reproduction photographie haute résolution, la forme matérielle change. Cela implique la question de la classification, comment stocker et référencer ces archives numérisées.
[^2]: À contrario, parmi les institutions culturelle, les bibliothèques ont été très rapides pour envisager l'apport conséquent que l'informatique pouvait apporter à leur domaine
[^3]: 
[^4]: C.f. la création de la grille et l'entrée des données, la partie 2.1.1 de ce mémoire
[^6]: prévu par Emmnauel et les AN? 
[^ 7]: Au-delà des potentielles erreurs de transcription, il pourrait y avoir des irrégularités dans la conversion du calendrier républicain par exemple.

